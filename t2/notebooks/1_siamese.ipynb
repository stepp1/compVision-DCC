{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7daafc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T22:43:34.087525Z",
     "start_time": "2021-05-28T22:43:34.078762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/step/Personal/UCH/2021-sem1/VisionComp/t2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "if Path.cwd().parent.stem == 't2':\n",
    "    %cd ..\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f082ea1",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337f3207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T22:43:35.216224Z",
     "start_time": "2021-05-28T22:43:34.088701Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2014041450\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(hash(\"setting a random seeds\") % 2**32 - 1)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4782770e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T22:45:56.409483Z",
     "start_time": "2021-05-28T22:45:56.404689Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import skimage.morphology as morph\n",
    "\n",
    "from typing import Tuple, Any\n",
    "\n",
    "class ErosionReplicate(object):\n",
    "    \"\"\"Applies Erosion to one channel and replicates the channel\"\"\"\n",
    "    def __call__(self, tensor):\n",
    "        one_channel = tensor[0, :, :]\n",
    "        one_channel = torch.from_numpy(morph.erosion(one_channel, morph.square(3)))    \n",
    "        tensor[0, :, :] = one_channel\n",
    "        tensor[1, :, :] = one_channel\n",
    "        tensor[2, :, :] = one_channel\n",
    "        return tensor\n",
    "        \n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, base_dir_sk: str,  base_dir_ph: str, triplet: bool = False):\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        # base_dirs\n",
    "        self.base_dir_sk = str(Path(base_dir_sk) / 'png_w256')\n",
    "        self.base_dir_ph = base_dir_ph \n",
    "        \n",
    "        # transforms\n",
    "        base_tr = [\n",
    "            transforms.Resize([224,224]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        sketch_tr = [ErosionReplicate()]\n",
    "        normalize_tr = [transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))]\n",
    "        \n",
    "        self.transforms_sk = transforms.Compose(base_tr + sketch_tr + normalize_tr)\n",
    "        self.transforms_ph = transforms.Compose(base_tr + normalize_tr)\n",
    "        \n",
    "        \n",
    "        # init two ImageFolders        \n",
    "        self.dataset_sk = datasets.ImageFolder(self.base_dir_sk)\n",
    "        self.dataset_ph = datasets.ImageFolder(self.base_dir_ph)\n",
    "        \n",
    "        self.n_classes = len(self.dataset_sk.classes)\n",
    "        self.idx_to_class = {idx: label for label, idx in self.dataset_sk.class_to_idx.items()}\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path_sk, target_sk = self.dataset_sk.samples[index]\n",
    "        path_ph, target_ph = self.dataset_ph.samples[index]\n",
    "        \n",
    "        sample_sk = self.dataset_sk.loader(path_sk)\n",
    "        sample_ph = self.dataset_ph.loader(path_ph)\n",
    "        \n",
    "        sample_sk = self.transforms_sk(sample_sk)\n",
    "        sample_ph = self.transforms_ph(sample_ph)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bcf2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T22:45:56.833817Z",
     "start_time": "2021-05-28T22:45:56.751117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SiameseDataset at 0x7f8ce4041eb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SiameseDataset(base_dir_ph='data/Flickr15K/', base_dir_sk='data/Sketch_EITZ/')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734fa1a",
   "metadata": {},
   "source": [
    "Contrastive Model        |  Triplet Model\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text](../informe/contrastive_arch.png \"Title\") | ![alt text](../informe/triplet_arch.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4deec51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T22:43:35.767360Z",
     "start_time": "2021-05-28T22:43:34.084Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, p: float = 2, dim: int = 1, eps: float = 1e-12):\n",
    "        self.p = p\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return F.normalize(input, self.p, self.dim, self.eps)\n",
    "\n",
    "\n",
    "class SimpleLearner(pl.LightningModule):\n",
    "    def __init__(self, models: Type[Union[nn.Module, dict]], n_classes: int, mode: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        # shared weights\n",
    "        shared_mlp = nn.Sequential(\n",
    "            nn.Linear(model.fc.in_features, 512), # fc1\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512) # fc2\n",
    "        )\n",
    "        \n",
    "        # shared weights for classification\n",
    "        classifier_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "        \n",
    "        # normalization before contrastive or triplet\n",
    "        siamese_out = nn.Sequential(\n",
    "            nn.Normalize()\n",
    "        )\n",
    "\n",
    "        ce_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if mode == 'contrastive':\n",
    "            siamese_criterion = ContrastiveLoss()\n",
    "\n",
    "        elif mode == 'triplet':\n",
    "            siamese_criterion = nn.TripletMarginLoss(0.5)\n",
    "            \n",
    "        self._losses = (ce_criterion, siamese_criterion)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if mode == 'contrastive':\n",
    "            # anchor is sketch, positive is photo\n",
    "            x_an, x_pos = x\n",
    "            emb_an = models['sketch'](x_an)\n",
    "            emb_pos = models['photo'](x_pos)\n",
    "        \n",
    "        elif mode == 'triplet':\n",
    "            # anchor is sketch, positive and negative is photo\n",
    "            x_anc, x_pos, x_neg = x\n",
    "            emb_anc = shared_mlp(models['sketch'](x_anc))\n",
    "            emb_pos = shared_mlp(models['photo'](x_pos))\n",
    "            emb_neg = shared_mlp(models['photo'](x_pos))\n",
    "            \n",
    "            n_emb_anc = torch.unsqueeze(siamese_out(emb_anc), 1)\n",
    "            n_emb_pos = torch.unsqueeze(siamese_out(emb_pos), 1)\n",
    "            n_emb_neg = torch.unsqueeze(siamese_out(emb_neg), 1)\n",
    "            \n",
    "                        \n",
    "            y_anc = torch.unsqueeze(classifier_mlp(emb_anc), 1)\n",
    "            y_pos = torch.unsqueeze(classifier_mlp(emb_pos), 1)\n",
    "            y_neg = torch.unsqueeze(classifier_mlp(emb_neg), 1)\n",
    "            \n",
    "            \n",
    "            embeddings = torch.cat((n_emb_anc, n_emb_pos, n_emb_neg), 1)\n",
    "            y_hats = torch.cat((y_anc, y_pos, y_neg), 1)\n",
    "            return embeddings, y_hats\n",
    "\n",
    "    def training_step(self, batch,  batch_idx):\n",
    "        samples, targets = batch[0], batch[1]\n",
    "        \n",
    "        embeddings, y_hats = self.forward(samples)\n",
    "        out = [embeddings, y_hats]\n",
    "        \n",
    "        losses = []\n",
    "        losses_names = []\n",
    "        for l, pred in zip(self._losses, out):\n",
    "            loss = l.loss(pred, samples)\n",
    "#             effective_loss = l.weight * loss\n",
    "\n",
    "            losses_names.append(l.name)\n",
    "            losses.append(loss)\n",
    "\n",
    "        losses_dict = {n: l for n, l in zip(losses_names, losses)}\n",
    "        if len(losses_names) > 1:\n",
    "            self.log(losses_dict, prog_bar=True, logger=False)\n",
    "        \n",
    "        losses_dict['loss'] = sum(losses)\n",
    "        return losses_dict\n",
    "\n",
    "    def validation_step(self, batch, batch_ixd):\n",
    "        img_lr = batch['lr']\n",
    "        img_hr = batch['hr']\n",
    "        img_sr = self.forward(img_lr)\n",
    "        \n",
    "        val_loss = self.criterion(img_sr, img_hr)\n",
    "        \n",
    "        self.log('val_loss', val_loss)\n",
    "        return {'val_loss':val_loss }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you define one or multiple optimizer, learning rate schedulers, etc.\n",
    "        # links provided on the train.py about setting up 2 opts for GAN\n",
    "        return [optim.Adam(self.parameters(), lr=PARAMS['learning_rate'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddeb4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
